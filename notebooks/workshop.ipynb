{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc344330",
   "metadata": {},
   "source": [
    "# 1: Cell imports & paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae587a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: c:\\Users\\Christian\\Desktop\\LangChain-Local-RA\\.venv\\Scripts\\python.exe\n",
      "cryptography: 46.0.2\n",
      "pypdf: 6.1.1\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Optional, Dict\n",
    "import hashlib\n",
    "import sys\n",
    "import chromadb\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader, UnstructuredURLLoader, PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaLLM, OllamaEmbeddings\n",
    "from pypdf.errors import WrongPasswordError\n",
    "\n",
    "import cryptography, pypdf\n",
    "\n",
    "print(\"Python:\", sys.executable)\n",
    "print(\"cryptography:\", cryptography.__version__)\n",
    "print(\"pypdf:\", pypdf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7169576d",
   "metadata": {},
   "source": [
    "# 2: Data Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3072ae81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: c:\\Users\\Christian\\Desktop\\LangChain-Local-RA\\notebooks\n",
      "DATA_DIR: C:\\Users\\Christian\\Desktop\\LangChain-Local-RA\\data\n",
      "DB_DIR: C:\\Users\\Christian\\Desktop\\LangChain-Local-RA\\chroma_db\n",
      "DATA_DIR contents: ['CalculusVolume1-OP.pdf']\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = Path(r\"C:\\Users\\Christian\\Desktop\\LangChain-Local-RA\\data\")\n",
    "DB_DIR   = Path(r\"C:\\Users\\Christian\\Desktop\\LangChain-Local-RA\\chroma_db\")\n",
    "\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"CWD:\", Path.cwd())\n",
    "print(\"DATA_DIR:\", DATA_DIR)\n",
    "print(\"DB_DIR:\", DB_DIR)\n",
    "print(\"DATA_DIR contents:\", [p.name for p in DATA_DIR.glob('*')])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d62a2a",
   "metadata": {},
   "source": [
    "# 3: Ollama Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e5315fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_MODEL   = \"llama3.1\"          # ollama pull llama3.1\n",
    "EMBED_MODEL = \"nomic-embed-text\"  # ollama pull nomic-embed-text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a10172",
   "metadata": {},
   "source": [
    "### 3a: Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6677d326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_DIR: C:\\Users\\Christian\\Desktop\\LangChain-Local-RA\\data\n",
      "DB_DIR: C:\\Users\\Christian\\Desktop\\LangChain-Local-RA\\chroma_db\n"
     ]
    }
   ],
   "source": [
    "print(\"DATA_DIR:\", DATA_DIR)\n",
    "print(\"DB_DIR:\", DB_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610c8325",
   "metadata": {},
   "source": [
    "# 4: Sanity Check\n",
    "- This will error if Ollama isn't running or the model isn't pulled yet.\n",
    "- In a terminal (outside Python), run:\n",
    "-   ollama pull llama3.1\n",
    "-   ollama pull nomic-embed-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f5753b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM and Embeddings ready\n",
      "DB present at: C:\\Users\\Christian\\Desktop\\LangChain-Local-RA\\chroma_db\\chroma.sqlite3 True\n"
     ]
    }
   ],
   "source": [
    "llm = OllamaLLM(model=LLM_MODEL, temperature=0.2)\n",
    "embeddings = OllamaEmbeddings(model=EMBED_MODEL)\n",
    "\n",
    "# Force Chroma to use the ROOT DB_DIR (outside notebooks)\n",
    "client = chromadb.PersistentClient(path=str(DB_DIR))  # <- hard pins the path\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    client=client,                             # <- use client, not persist_directory\n",
    "    collection_name=\"local_research_assistant\",\n",
    "    embedding_function=embeddings,\n",
    ")\n",
    "\n",
    "print(\"LLM and Embeddings ready\")\n",
    "# sanity check: confirm the DB file is in the right place\n",
    "print(\"DB present at:\", DB_DIR / \"chroma.sqlite3\", (DB_DIR / \"chroma.sqlite3\").exists())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b924cd1",
   "metadata": {},
   "source": [
    "# 5: Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a707f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If any PDF is password-protected, add it here:\n",
    "PDF_PASSWORDS: Dict[str, Optional[str]] = {\n",
    "    # \"SomeLockedFile.pdf\": \"your_password_here\",\n",
    "}\n",
    "\n",
    "def load_pdf_pages_robust(pdf_path: Path) -> List:\n",
    "    \"\"\"Load a PDF via PyMuPDF, fall back to PyPDF. Sets absolute 'source' in metadata.\"\"\"\n",
    "    pages: List = []\n",
    "    pwd = PDF_PASSWORDS.get(pdf_path.name)\n",
    "\n",
    "    # 1) PyMuPDF (best text extraction on technical PDFs)\n",
    "    try:\n",
    "        pages = PyMuPDFLoader(str(pdf_path)).load()\n",
    "        for d in pages:\n",
    "            d.metadata[\"source\"] = str(pdf_path.resolve())\n",
    "        if pages:\n",
    "            return pages\n",
    "    except Exception as e:\n",
    "        print(f\"PyMuPDF failed for {pdf_path.name}: {e}\")\n",
    "\n",
    "    # 2) Fallback: PyPDF (needs 'cryptography' for encrypted files)\n",
    "    try:\n",
    "        pages = PyPDFLoader(str(pdf_path), password=pwd).load()\n",
    "        for d in pages:\n",
    "            d.metadata[\"source\"] = str(pdf_path.resolve())\n",
    "        if pages:\n",
    "            return pages\n",
    "    except WrongPasswordError:\n",
    "        print(f\"Skipping encrypted PDF (password needed): {pdf_path.name}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"PyPDF failed for {pdf_path.name}: {e}\")\n",
    "\n",
    "    return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f83a60",
   "metadata": {},
   "source": [
    "# 6: Load Documents (PDF's and/or URLs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971b016d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDFs found: ['CalculusVolume1-OP.pdf']\n",
      "Loaded 873 pages across 1 PDF(s).\n"
     ]
    }
   ],
   "source": [
    "# Dedupe across *.pdf and *.PDF\n",
    "pdf_paths = sorted({str(p.resolve()) for p in DATA_DIR.glob(\"*.pdf\")} |\n",
    "                    {str(p.resolve()) for p in DATA_DIR.glob(\"*.PDF\")})\n",
    "pdfs = [Path(p) for p in pdf_paths]\n",
    "print(\"PDFs found:\", [p.name for p in pdfs])\n",
    "\n",
    "raw_docs: List = []\n",
    "for p in pdfs:\n",
    "    raw_docs.extend(load_pdf_pages_robust(p))\n",
    "\n",
    "# Optional web pages (if you need them)\n",
    "urls: list[str] = []  # e.g., [\"https://langchain.readthedocs.io/\"]\n",
    "# raw_docs += UnstructuredURLLoader(urls=urls, continue_on_failure=True).load()\n",
    "\n",
    "print(f\"Loaded {len(raw_docs)} pages across {len(pdfs)} PDF(s).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540e2735",
   "metadata": {},
   "source": [
    "# 7: Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12297c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 1543 chunks.\n"
     ]
    }
   ],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1200,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    add_start_index=True,\n",
    ")\n",
    "docs = splitter.split_documents(raw_docs)\n",
    "print(f\"Split into {len(docs)} chunks.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692240d4",
   "metadata": {},
   "source": [
    "# 8: Vector Store (Create or Reuse; initial add only if empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a763e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed documents (auto-persisted).\n"
     ]
    }
   ],
   "source": [
    "has_index = len(vectorstore.get().get(\"ids\", [])) > 0\n",
    "if (not has_index) and len(docs) > 0:\n",
    "    vectorstore.add_documents(docs)  # auto-persist; no .persist()\n",
    "    print(\"Indexed documents (auto-persisted).\")\n",
    "else:\n",
    "    print(\"Using existing index (or no docs to add).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f70c32",
   "metadata": {},
   "source": [
    "# 9: Folder-wide sync (add new, remove deleted) â€” public API only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8a7938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _chunk_id(doc):\n",
    "    src  = str(doc.metadata.get(\"source\",\"\"))\n",
    "    page = str(doc.metadata.get(\"page\",\"\"))\n",
    "    head = doc.page_content[:200].encode(\"utf-8\", errors=\"ignore\")\n",
    "    return f\"{src}::p{page}::{hashlib.md5(head).hexdigest()}\"\n",
    "\n",
    "def sync_all_pdfs():\n",
    "    # Current files in folder\n",
    "    current_files = {str(p.resolve()) for p in (list(DATA_DIR.glob(\"*.pdf\")) + list(DATA_DIR.glob(\"*.PDF\")))}\n",
    "\n",
    "    # What's in Chroma now â€” request a VALID include (ids will still be in the result)\n",
    "    all_data = vectorstore.get(include=[\"metadatas\"])  # âœ… no \"ids\" here\n",
    "    existing_ids = set(all_data.get(\"ids\", []))        # ids are still present in the return\n",
    "    metas = all_data.get(\"metadatas\", [])\n",
    "    indexed_sources = {m.get(\"source\") for m in metas if m.get(\"source\")}\n",
    "\n",
    "    # 1) Remove chunks for PDFs that were deleted\n",
    "    removed_sources = {s for s in indexed_sources if s not in current_files}\n",
    "    if removed_sources:\n",
    "        print(f\"ðŸ§¹ Removing chunks for deleted PDFs: {[Path(s).name for s in removed_sources]}\")\n",
    "        for s in removed_sources:\n",
    "            vectorstore.delete(where={\"source\": s})\n",
    "    else:\n",
    "        print(\"âœ… No deleted PDFs found.\")\n",
    "\n",
    "    # 2) Add new/changed PDFs\n",
    "    added = 0\n",
    "    for pdf in sorted(current_files):\n",
    "        pages = load_pdf_pages_robust(Path(pdf))\n",
    "        if not pages:\n",
    "            continue\n",
    "        chunks = splitter.split_documents(pages)\n",
    "        ids = [_chunk_id(c) for c in chunks]\n",
    "        new_docs, new_ids = [], []\n",
    "        for d, cid in zip(chunks, ids):\n",
    "            if cid not in existing_ids:\n",
    "                new_docs.append(d); new_ids.append(cid)\n",
    "        if new_docs:\n",
    "            vectorstore.add_documents(new_docs, ids=new_ids)  # auto-persist\n",
    "            existing_ids.update(new_ids)\n",
    "            added += len(new_docs)\n",
    "\n",
    "    print(f\"Sync complete. Added {added} new chunk(s).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51a3e54",
   "metadata": {},
   "source": [
    "# 9b: Optional â€” Clear and Resync Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a905f895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¹ Cleared 1543 vectors from Chroma.\n",
      "âœ… No deleted PDFs found.\n",
      "âœ… Sync complete. Added 1543 new chunk(s).\n"
     ]
    }
   ],
   "source": [
    "def clear_chroma_all():\n",
    "    data = vectorstore.get(include=[\"metadatas\"])  # valid include\n",
    "    ids = data.get(\"ids\", [])\n",
    "    if not ids:\n",
    "        print(\"No vectors to delete.\")\n",
    "        return\n",
    "    vectorstore.delete(ids=ids)\n",
    "    print(f\"Cleared {len(ids)} vectors from Chroma.\")\n",
    "\n",
    "clear_chroma_all()\n",
    "sync_all_pdfs()  # rebuild from PDFs currently in DATA_DIR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd6685c",
   "metadata": {},
   "source": [
    "# 10: Ask helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c8fb98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "def ask(question: str) -> str:\n",
    "    rel_docs = retriever.invoke(question)  # new API\n",
    "    context = \"\\n\\n---\\n\\n\".join(\n",
    "        f\"[{Path(d.metadata.get('source','?')).name} p{d.metadata.get('page','?')}] {d.page_content[:1000]}\"\n",
    "        for d in rel_docs\n",
    "    )\n",
    "    prompt = (\n",
    "        \"You are a helpful research assistant. Answer strictly from the context. \"\n",
    "        \"If unsure, say you don't know. Add brief page refs.\\n\\n\"\n",
    "        f\"Q: {question}\\n\\nContext:\\n{context}\\n\\nA:\"\n",
    "    )\n",
    "    return llm.invoke(prompt)\n",
    "\n",
    "def ask_fresh(question: str) -> str:\n",
    "    sync_all_pdfs()   # keep index in sync with folder\n",
    "    return ask(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0eca00",
   "metadata": {},
   "source": [
    "# 11: Quick checks / examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f861e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDFs in data/: ['CalculusVolume1-OP.pdf']\n",
      "Indexed chunks: 1543\n",
      "I'll explain each method described in the pdf.\n",
      "\n",
      "**Newton's Method**\n",
      "\n",
      "Newton's method is an iterative process used to find roots of a function f(x). It starts with an initial guess x0 and uses the formula:\n",
      "\n",
      "xn = F(xn-1) = xn-1 - [f(xn-1)/f'(xn-1)]\n",
      "\n",
      "where f'(x) is the derivative of f(x).\n",
      "\n",
      "In the given example, we use Newton's method to find the root of f(x) = x^2 - 3 with an initial guess x0 = 3. We calculate x1 and x2 using the formula above.\n",
      "\n",
      "**Iterative Process**\n",
      "\n",
      "An iterative process is a type of process where each term is defined in terms of the previous term by repeating the same function. In Newton's method, we define F(x) = x - [f(x)/f'(x)], which is an example of an iterative process.\n",
      "\n",
      "**Failures of Newton's Method**\n",
      "\n",
      "Newton's method can fail to find a root if:\n",
      "\n",
      "1. The derivative f'(x) is zero at xn, but f(xn) â‰  0. This means that the tangent line of f at xn does not intersect the x-axis, so we cannot use it as an approximation for the root.\n",
      "\n",
      "**Other Methods**\n",
      "\n",
      "Unfortunately, I don't know what other methods are described in the pdf beyond Newton's method and iterative processes.\n"
     ]
    }
   ],
   "source": [
    "print(\"PDFs in data/:\", [p.name for p in DATA_DIR.glob(\"*.pdf\")])\n",
    "print(\"Indexed chunks:\", len((vectorstore.get() or {}).get(\"ids\", [])))\n",
    "\n",
    "# Example queries:\n",
    "ans = ask(\"EXplain each method that is described in the pdf.\")\n",
    "print(ans)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lc-ra-312)",
   "language": "python",
   "name": "lc-ra-312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
