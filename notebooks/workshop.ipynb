{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc344330",
   "metadata": {},
   "source": [
    "# 1: Cell imports & paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae587a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: c:\\Users\\Christian\\Desktop\\LangChain-Local-RA\\.venv\\Scripts\\python.exe\n",
      "cryptography: 46.0.2\n",
      "pypdf: 6.1.1\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Optional, Dict\n",
    "import hashlib\n",
    "import sys\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader, UnstructuredURLLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaLLM, OllamaEmbeddings\n",
    "\n",
    "import cryptography, pypdf\n",
    "\n",
    "print(\"Python:\", sys.executable)\n",
    "print(\"cryptography:\", cryptography.__version__)\n",
    "print(\"pypdf:\", pypdf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7169576d",
   "metadata": {},
   "source": [
    "# 2: Data Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3072ae81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_DIR: C:\\Users\\Christian\\Desktop\\LangChain-Local-RA\\notebooks\\data\n",
      "DB_DIR: C:\\Users\\Christian\\Desktop\\LangChain-Local-RA\\notebooks\\chroma_db\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = Path(\"data\").resolve()      # put your PDFs here\n",
    "DB_DIR   = Path(\"chroma_db\").resolve() # Chroma persistence\n",
    "\n",
    "DATA_DIR.mkdir(exist_ok=True, parents=True)\n",
    "DB_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(\"DATA_DIR:\", DATA_DIR)\n",
    "print(\"DB_DIR:\", DB_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d62a2a",
   "metadata": {},
   "source": [
    "# 3: Ollama Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e5315fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_MODEL   = \"llama3.1\"          # ollama pull llama3.1\n",
    "EMBED_MODEL = \"nomic-embed-text\"  # ollama pull nomic-embed-text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a10172",
   "metadata": {},
   "source": [
    "### 3a: Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6677d326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_DIR: C:\\Users\\Christian\\Desktop\\LangChain-Local-RA\\notebooks\\data\n",
      "DB_DIR: C:\\Users\\Christian\\Desktop\\LangChain-Local-RA\\notebooks\\chroma_db\n"
     ]
    }
   ],
   "source": [
    "print(\"DATA_DIR:\", DATA_DIR)\n",
    "print(\"DB_DIR:\", DB_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610c8325",
   "metadata": {},
   "source": [
    "# 4: Sanity Check\n",
    "- This will error if Ollama isn't running or the model isn't pulled yet.\n",
    "- In a terminal (outside Python), run:\n",
    "-   ollama pull llama3.1\n",
    "-   ollama pull nomic-embed-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f5753b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM and Embeddings ready\n"
     ]
    }
   ],
   "source": [
    "\n",
    "llm = OllamaLLM(model=LLM_MODEL, temperature=0.2)\n",
    "embeddings = OllamaEmbeddings(model=EMBED_MODEL)\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"local_research_assistant\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=str(DB_DIR),  # auto-persist on writes\n",
    ")\n",
    "\n",
    "print(\"LLM and Embeddings ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b924cd1",
   "metadata": {},
   "source": [
    "# 5: Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58a707f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf.errors import WrongPasswordError\n",
    "\n",
    "# If you have passwords for specific encrypted PDFs, add them here:\n",
    "PDF_PASSWORDS: Dict[str, Optional[str]] = {\n",
    "    # \"LockedFile.pdf\": \"your_password_here\",\n",
    "}\n",
    "\n",
    "def load_pdf_pages(path: Path):\n",
    "    \"\"\"Load a single PDF; skip gracefully if encrypted or unreadable.\"\"\"\n",
    "    pwd = PDF_PASSWORDS.get(path.name)\n",
    "    try:\n",
    "        return PyPDFLoader(str(path), password=pwd).load()\n",
    "    except WrongPasswordError:\n",
    "        print(f\"🔒 Skipping encrypted PDF (needs password): {path.name}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Could not load {path.name}: {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f83a60",
   "metadata": {},
   "source": [
    "# 6: Load Documents (PDF's and/or URLs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "971b016d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 PDF(s). Loaded 0 raw docs.\n"
     ]
    }
   ],
   "source": [
    "pdfs = list(DATA_DIR.glob(\"*.pdf\"))\n",
    "\n",
    "# Optionally add web pages:\n",
    "urls: list[str] = []  # e.g., [\"https://langchain.readthedocs.io/\"]\n",
    "\n",
    "raw_docs: List = []\n",
    "for p in pdfs:\n",
    "    raw_docs.extend(load_pdf_pages(p))\n",
    "\n",
    "# If you want to load URLs too, uncomment:\n",
    "# raw_docs += UnstructuredURLLoader(urls=urls, continue_on_failure=True).load()\n",
    "\n",
    "print(f\"Found {len(pdfs)} PDF(s). Loaded {len(raw_docs)} raw docs.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540e2735",
   "metadata": {},
   "source": [
    "# 7: Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12297c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 0 chunks.\n"
     ]
    }
   ],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1200,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    add_start_index=True,\n",
    ")\n",
    "docs = splitter.split_documents(raw_docs)\n",
    "print(f\"Split into {len(docs)} chunks.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692240d4",
   "metadata": {},
   "source": [
    "# 8: Vector Store (Create or Reuse; initial add only if empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a763e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing index (or no docs to add).\n"
     ]
    }
   ],
   "source": [
    "# 8: Vector Store (Create or Reuse; initial add only if empty)\n",
    "\n",
    "has_index = len(vectorstore.get().get(\"ids\", [])) > 0\n",
    "if (not has_index) and len(docs) > 0:\n",
    "    vectorstore.add_documents(docs)  # auto-persist; no .persist()\n",
    "    print(\"Indexed documents (auto-persisted).\")\n",
    "else:\n",
    "    print(\"Using existing index (or no docs to add).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f70c32",
   "metadata": {},
   "source": [
    "# 9: Folder-wide sync (idempotent; skips duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b8a7938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _chunk_id(doc):\n",
    "    \"\"\"Stable ID per chunk: source path + page + hash(first 200 chars).\"\"\"\n",
    "    src  = str(doc.metadata.get(\"source\",\"\"))\n",
    "    page = str(doc.metadata.get(\"page\",\"\"))\n",
    "    head = doc.page_content[:200].encode(\"utf-8\", errors=\"ignore\")\n",
    "    return f\"{src}::p{page}::{hashlib.md5(head).hexdigest()}\"\n",
    "\n",
    "def sync_all_pdfs():\n",
    "    pdf_paths = list(DATA_DIR.glob(\"*.pdf\"))\n",
    "    if not pdf_paths:\n",
    "        print(\"No PDFs found in data/. Drop files there and rerun.\")\n",
    "        return\n",
    "\n",
    "    existing_ids = set(vectorstore.get().get(\"ids\", []))\n",
    "    added, total = 0, 0\n",
    "\n",
    "    for pdf in pdf_paths:\n",
    "        pages = load_pdf_pages(pdf)\n",
    "        if not pages:\n",
    "            continue\n",
    "\n",
    "        chunks = splitter.split_documents(pages)\n",
    "        total += len(chunks)\n",
    "\n",
    "        ids = [_chunk_id(c) for c in chunks]\n",
    "        new_docs, new_ids = [], []\n",
    "        for d, cid in zip(chunks, ids):\n",
    "            if cid not in existing_ids:\n",
    "                new_docs.append(d); new_ids.append(cid)\n",
    "\n",
    "        if new_docs:\n",
    "            vectorstore.add_documents(new_docs, ids=new_ids)  # auto-persist\n",
    "            existing_ids.update(new_ids)\n",
    "            added += len(new_docs)\n",
    "\n",
    "    print(f\"✅ Synced {len(pdf_paths)} PDF(s). Added {added} new chunk(s) (of {total} total).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd6685c",
   "metadata": {},
   "source": [
    "# 10: Ask helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c8fb98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "def ask(question: str) -> str:\n",
    "    rel_docs = retriever.get_relevant_documents(question)\n",
    "    context = \"\\n\\n---\\n\\n\".join(\n",
    "        f\"[{d.metadata.get('source','?')} p{d.metadata.get('page','?')}] {d.page_content[:1000]}\"\n",
    "        for d in rel_docs\n",
    "    )\n",
    "    prompt = (\n",
    "        \"You are a helpful research assistant. Answer strictly from the context. \"\n",
    "        \"If unsure, say you don't know. Add brief page refs.\\n\\n\"\n",
    "        f\"Q: {question}\\n\\nContext:\\n{context}\\n\\nA:\"\n",
    "    )\n",
    "    return llm.invoke(prompt)\n",
    "\n",
    "def ask_fresh(question: str) -> str:\n",
    "    sync_all_pdfs()   # quick incremental sync; no duplicates re-embedded\n",
    "    return ask(question)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0eca00",
   "metadata": {},
   "source": [
    "# 11: Quick checks / examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f861e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDFs in data/: []\n",
      "Indexed chunks: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"PDFs in data/:\", [p.name for p in DATA_DIR.glob(\"*.pdf\")])\n",
    "print(\"Indexed chunks:\", len((vectorstore.get() or {}).get(\"ids\", [])))\n",
    "\n",
    "# Example queries:\n",
    "# ans = ask(\"List the key study tactics and exam strategies across all PDFs.\")\n",
    "# print(ans)\n",
    "\n",
    "# ans = ask_fresh(\"Summarize memory techniques mentioned.\")\n",
    "# print(ans)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lc-ra-desktop)",
   "language": "python",
   "name": "lc-ra-desktop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
