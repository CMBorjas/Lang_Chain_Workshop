{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc344330",
   "metadata": {},
   "source": [
    "# 1: Cell imports & paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae587a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader, UnstructuredURLLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaLLM, OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7169576d",
   "metadata": {},
   "source": [
    "# 2: Data Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3072ae81",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"../data\").resolve()\n",
    "DB_DIR = Path(\"../chroma_db\").resolve()\n",
    "DB_DIR.mkdir(exist_ok=True, parents=True)\n",
    "DATA_DIR.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d62a2a",
   "metadata": {},
   "source": [
    "# 3: Ollama Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e5315fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_MODEL = \"llama3.1\"\n",
    "EMBED_MODEL = \"nomic-embed-text\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a10172",
   "metadata": {},
   "source": [
    "### 3a: Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6677d326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_DIR: C:\\Users\\Christian\\OneDrive - The University of Colorado Denver\\LangChain-Local-RA\\data\n",
      "DB_DIR: C:\\Users\\Christian\\OneDrive - The University of Colorado Denver\\LangChain-Local-RA\\chroma_db\n"
     ]
    }
   ],
   "source": [
    "print(\"DATA_DIR:\", DATA_DIR)\n",
    "print(\"DB_DIR:\", DB_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610c8325",
   "metadata": {},
   "source": [
    "# 4: Sanity Check\n",
    "- This will error if Ollama isn't running or the model isn't pulled yet.\n",
    "- In a terminal (outside Python), run:\n",
    "-   ollama pull llama3.1\n",
    "-   ollama pull nomic-embed-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f5753b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM and Embeddings ready ✅\n"
     ]
    }
   ],
   "source": [
    "\n",
    "llm = OllamaLLM(model=LLM_MODEL, temperature=0.2)\n",
    "embeddings = OllamaEmbeddings(model=EMBED_MODEL)\n",
    "print(\"LLM and Embeddings ready ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f83a60",
   "metadata": {},
   "source": [
    "# 5: Load Documents (PDF's and/or URLs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971b016d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 0 raw docs\n"
     ]
    }
   ],
   "source": [
    "def load_pdfs(pdf_paths: List[Path]):\n",
    "    docs = []\n",
    "    for p in pdf_paths:\n",
    "        if p.exists() and p.suffix.lower() == \".pdf\":\n",
    "            docs.extend(PyPDFLoader(str(p)).load())\n",
    "    return docs\n",
    "\n",
    "def load_urls(urls: List[str]):\n",
    "    if not urls:\n",
    "        return []\n",
    "    return UnstructuredURLLoader(urls=urls, continue_on_failure=True).load()\n",
    "\n",
    "# Put any PDFs into ../data/ and they’ll be picked up automatically\n",
    "pdfs = list(DATA_DIR.glob(\"*.pdf\"))\n",
    "urls = []  # e.g., [\"https://langchain.readthedocs.io/\"]\n",
    "\n",
    "raw_docs = load_pdfs(pdfs) + load_urls(urls)\n",
    "print(f\"Loaded {len(raw_docs)} raw docs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b924cd1",
   "metadata": {},
   "source": [
    "# 6: Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a707f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 0 chunks\n"
     ]
    }
   ],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1200,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    add_start_index=True,\n",
    ")\n",
    "docs = splitter.split_documents(raw_docs)\n",
    "print(f\"Split into {len(docs)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540e2735",
   "metadata": {},
   "source": [
    "# 7: Vector Store (Create or Reuse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12297c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing index (or no docs to add).\n"
     ]
    }
   ],
   "source": [
    "vectorstore = Chroma(\n",
    "    collection_name=\"local_research_assistant\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=str(DB_DIR),\n",
    ")\n",
    "\n",
    "# Only add if index is empty and we have docs\n",
    "needs_index = (len(vectorstore.get().get(\"ids\", [])) == 0) and len(docs) > 0\n",
    "if needs_index:\n",
    "    vectorstore.add_documents(docs)\n",
    "    vectorstore.persist()\n",
    "    print(\"Indexed and persisted documents.\")\n",
    "else:\n",
    "    print(\"Using existing index (or no docs to add).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692240d4",
   "metadata": {},
   "source": [
    "# 8: Ask questions to the helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a763e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "def ask(q: str) -> str:\n",
    "    rel_docs = retriever.get_relevant_documents(q)\n",
    "    # Build a compact context with source + page ref\n",
    "    ctx = \"\\n\\n---\\n\\n\".join(\n",
    "        f\"[{d.metadata.get('source','?')} p{d.metadata.get('page','?')}] {d.page_content[:1000]}\"\n",
    "        for d in rel_docs\n",
    "    )\n",
    "    prompt = (\n",
    "        \"You are a helpful research assistant. Answer strictly from the context.\\n\"\n",
    "        \"If unsure, say you don't know. Add brief page refs.\\n\\n\"\n",
    "        f\"Q: {q}\\n\\nContext:\\n{ctx}\\n\\nA:\"\n",
    "    )\n",
    "    return llm.invoke(prompt)\n",
    "\n",
    "# Try it (after adding a PDF):\n",
    "# ask(\"Give me a 3-sentence summary with page references.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
