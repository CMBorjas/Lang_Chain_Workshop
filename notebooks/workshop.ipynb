{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc344330",
   "metadata": {},
   "source": [
    "# 1: Cell imports & paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae587a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: c:\\Users\\Christian\\Desktop\\LangChain-Local-RA\\.venv\\Scripts\\python.exe\n",
      "cryptography: 46.0.2\n",
      "pypdf: 6.1.1\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Optional, Dict\n",
    "import hashlib\n",
    "import sys\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader, UnstructuredURLLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaLLM, OllamaEmbeddings\n",
    "\n",
    "import cryptography, pypdf\n",
    "\n",
    "print(\"Python:\", sys.executable)\n",
    "print(\"cryptography:\", cryptography.__version__)\n",
    "print(\"pypdf:\", pypdf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7169576d",
   "metadata": {},
   "source": [
    "# 2: Data Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3072ae81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: c:\\Users\\Christian\\Desktop\\LangChain-Local-RA\\notebooks\n",
      "DATA_DIR: C:\\Users\\Christian\\Desktop\\LangChain-Local-RA\\data\n",
      "DB_DIR: C:\\Users\\Christian\\Desktop\\LangChain-Local-RA\\chroma_db\n",
      "DATA_DIR contents: ['Fundamentals-of-Parallel-Processing-chapters1-5_22-43.pdf', 'Spark Charts - Study Tactics.pdf']\n"
     ]
    }
   ],
   "source": [
    "# 2: Data Directories (fixed absolute path)\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path(r\"C:\\Users\\Christian\\Desktop\\LangChain-Local-RA\\data\")\n",
    "DB_DIR   = Path(r\"C:\\Users\\Christian\\Desktop\\LangChain-Local-RA\\chroma_db\")\n",
    "\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"CWD:\", Path.cwd())\n",
    "print(\"DATA_DIR:\", DATA_DIR)\n",
    "print(\"DB_DIR:\", DB_DIR)\n",
    "print(\"DATA_DIR contents:\", [p.name for p in DATA_DIR.glob('*')])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d62a2a",
   "metadata": {},
   "source": [
    "# 3: Ollama Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e5315fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_MODEL   = \"llama3.1\"          # ollama pull llama3.1\n",
    "EMBED_MODEL = \"nomic-embed-text\"  # ollama pull nomic-embed-text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a10172",
   "metadata": {},
   "source": [
    "### 3a: Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6677d326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_DIR: C:\\Users\\Christian\\Desktop\\LangChain-Local-RA\\notebooks\\data\n",
      "DB_DIR: C:\\Users\\Christian\\Desktop\\LangChain-Local-RA\\notebooks\\chroma_db\n"
     ]
    }
   ],
   "source": [
    "print(\"DATA_DIR:\", DATA_DIR)\n",
    "print(\"DB_DIR:\", DB_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610c8325",
   "metadata": {},
   "source": [
    "# 4: Sanity Check\n",
    "- This will error if Ollama isn't running or the model isn't pulled yet.\n",
    "- In a terminal (outside Python), run:\n",
    "-   ollama pull llama3.1\n",
    "-   ollama pull nomic-embed-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f5753b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM and Embeddings ready\n"
     ]
    }
   ],
   "source": [
    "\n",
    "llm = OllamaLLM(model=LLM_MODEL, temperature=0.2)\n",
    "embeddings = OllamaEmbeddings(model=EMBED_MODEL)\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"local_research_assistant\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=str(DB_DIR),  # auto-persist on writes\n",
    ")\n",
    "\n",
    "print(\"LLM and Embeddings ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b924cd1",
   "metadata": {},
   "source": [
    "# 5: Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58a707f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf.errors import WrongPasswordError\n",
    "\n",
    "# If you have passwords for specific encrypted PDFs, add them here:\n",
    "PDF_PASSWORDS: Dict[str, Optional[str]] = {\n",
    "    # \"LockedFile.pdf\": \"your_password_here\",\n",
    "}\n",
    "\n",
    "def load_pdf_pages(path: Path):\n",
    "    \"\"\"Load a single PDF; skip gracefully if encrypted or unreadable.\"\"\"\n",
    "    pwd = PDF_PASSWORDS.get(path.name)\n",
    "    try:\n",
    "        return PyPDFLoader(str(path), password=pwd).load()\n",
    "    except WrongPasswordError:\n",
    "        print(f\"Skipping encrypted PDF (needs password): {path.name}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load {path.name}: {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f83a60",
   "metadata": {},
   "source": [
    "# 6: Load Documents (PDF's and/or URLs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "971b016d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 PDF(s). Loaded 52 raw docs.\n",
      "PDFs in data/: ['Fundamentals-of-Parallel-Processing-chapters1-5_22-43.pdf', 'Spark Charts - Study Tactics.pdf', 'Fundamentals-of-Parallel-Processing-chapters1-5_22-43.pdf', 'Spark Charts - Study Tactics.pdf']\n"
     ]
    }
   ],
   "source": [
    "pdfs = list(DATA_DIR.glob(\"*.pdf\")) + list(DATA_DIR.glob(\"*.PDF\"))  # case-insensitive\n",
    "\n",
    "# Optionally add web pages:\n",
    "urls: list[str] = []  # e.g., [\"https://langchain.readthedocs.io/\"]\n",
    "\n",
    "raw_docs: List = []\n",
    "for p in pdfs:\n",
    "    raw_docs.extend(load_pdf_pages(p))\n",
    "\n",
    "# If you want to load URLs too, uncomment:\n",
    "# raw_docs += UnstructuredURLLoader(urls=urls, continue_on_failure=True).load()\n",
    "\n",
    "print(f\"Found {len(pdfs)} PDF(s). Loaded {len(raw_docs)} raw docs.\")\n",
    "print(\"PDFs in data/:\", [p.name for p in pdfs])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540e2735",
   "metadata": {},
   "source": [
    "# 7: Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12297c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 326 chunks.\n"
     ]
    }
   ],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1200,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    add_start_index=True,\n",
    ")\n",
    "docs = splitter.split_documents(raw_docs)\n",
    "print(f\"Split into {len(docs)} chunks.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692240d4",
   "metadata": {},
   "source": [
    "# 8: Vector Store (Create or Reuse; initial add only if empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a763e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed documents (auto-persisted).\n"
     ]
    }
   ],
   "source": [
    "has_index = len(vectorstore.get().get(\"ids\", [])) > 0\n",
    "if (not has_index) and len(docs) > 0:\n",
    "    vectorstore.add_documents(docs)  # auto-persist; no .persist()\n",
    "    print(\"Indexed documents (auto-persisted).\")\n",
    "else:\n",
    "    print(\"Using existing index (or no docs to add).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f70c32",
   "metadata": {},
   "source": [
    "# 9: Folder-wide sync (idempotent; skips duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b8a7938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _chunk_id(doc):\n",
    "    \"\"\"Stable ID per chunk: source path + page + hash(first 200 chars).\"\"\"\n",
    "    src  = str(doc.metadata.get(\"source\",\"\"))\n",
    "    page = str(doc.metadata.get(\"page\",\"\"))\n",
    "    head = doc.page_content[:200].encode(\"utf-8\", errors=\"ignore\")\n",
    "    return f\"{src}::p{page}::{hashlib.md5(head).hexdigest()}\"\n",
    "\n",
    "def sync_all_pdfs():\n",
    "    pdf_paths = list(DATA_DIR.glob(\"*.pdf\"))\n",
    "    if not pdf_paths:\n",
    "        print(\"No PDFs found in data/. Drop files there and rerun.\")\n",
    "        return\n",
    "\n",
    "    existing_ids = set(vectorstore.get().get(\"ids\", []))\n",
    "    added, total = 0, 0\n",
    "\n",
    "    for pdf in pdf_paths:\n",
    "        pages = load_pdf_pages(pdf)\n",
    "        if not pages:\n",
    "            continue\n",
    "\n",
    "        chunks = splitter.split_documents(pages)\n",
    "        total += len(chunks)\n",
    "\n",
    "        ids = [_chunk_id(c) for c in chunks]\n",
    "        new_docs, new_ids = [], []\n",
    "        for d, cid in zip(chunks, ids):\n",
    "            if cid not in existing_ids:\n",
    "                new_docs.append(d); new_ids.append(cid)\n",
    "\n",
    "        if new_docs:\n",
    "            vectorstore.add_documents(new_docs, ids=new_ids)  # auto-persist\n",
    "            existing_ids.update(new_ids)\n",
    "            added += len(new_docs)\n",
    "\n",
    "    print(f\"✅ Synced {len(pdf_paths)} PDF(s). Added {added} new chunk(s) (of {total} total).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd6685c",
   "metadata": {},
   "source": [
    "# 10: Ask helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c8fb98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "def ask(question: str) -> str:\n",
    "    rel_docs = retriever.get_relevant_documents(question)\n",
    "    context = \"\\n\\n---\\n\\n\".join(\n",
    "        f\"[{d.metadata.get('source','?')} p{d.metadata.get('page','?')}] {d.page_content[:1000]}\"\n",
    "        for d in rel_docs\n",
    "    )\n",
    "    prompt = (\n",
    "        \"You are a helpful research assistant. Answer strictly from the context. \"\n",
    "        \"If unsure, say you don't know. Add brief page refs.\\n\\n\"\n",
    "        f\"Q: {question}\\n\\nContext:\\n{context}\\n\\nA:\"\n",
    "    )\n",
    "    return llm.invoke(prompt)\n",
    "\n",
    "def ask_fresh(question: str) -> str:\n",
    "    sync_all_pdfs()   # quick incremental sync; no duplicates re-embedded\n",
    "    return ask(question)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0eca00",
   "metadata": {},
   "source": [
    "# 11: Quick checks / examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f861e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDFs in data/: ['Fundamentals-of-Parallel-Processing-chapters1-5_22-43.pdf', 'Spark Charts - Study Tactics.pdf']\n",
      "Indexed chunks: 326\n",
      "Based on the provided PDFs, here are the key study tactics and exam strategies mentioned:\n",
      "\n",
      "**Study Tactics:**\n",
      "\n",
      "1. **SQ3R Method**: Survey, Question, Read, Recite, Review (p2)\n",
      "2. **PQRST Method**: Preview, Question, Read, Summarize, Test (p2)\n",
      "3. **Chunking**: Break down large amounts of information into smaller chunks (p2)\n",
      "4. **Mnemonics**: Use associations and acronyms to remember key terms and concepts (p2)\n",
      "5. **Self-Testing**: Test yourself on the material to identify areas for improvement (p3)\n",
      "\n",
      "**Exam Strategies:**\n",
      "\n",
      "1. **Read the Question Carefully**: Understand what is being asked before answering (p3)\n",
      "2. **Use Process of Elimination**: Eliminate obviously incorrect answers and focus on the remaining options (p3)\n",
      "3. **Manage Your Time Effectively**: Allocate time wisely to complete all questions within the allotted time (p3)\n",
      "\n",
      "Please note that these are the only study tactics and exam strategies mentioned in the provided PDFs.\n",
      "✅ Synced 2 PDF(s). Added 163 new chunk(s) (of 163 total).\n",
      "The text does not mention specific \"memory techniques\" but rather discusses parallel processing architectures and programming concepts.\n",
      "\n",
      "However, it does mention two types of parallelism:\n",
      "\n",
      "1. **SIMD (Single Instruction, Multiple Data) parallelism**: This involves applying multiple independent instructions to multiple data values in parallel. The foremost issue in SIMD programming is specifying operations on vectors at the statement level.\n",
      "2. **MIMD (Multiple Instruction, Multiple Data) parallelism**: This involves creating and coordinating multiple sequential instruction streams that share data. MIMD parallelism can be thought of as asynchronous parallelism.\n",
      "\n",
      "Additionally, the text mentions some pseudocode extensions for multiprocessor programming, including:\n",
      "\n",
      "* `fork` and `join` statements to create and coordinate multiple processes\n",
      "* `shared` and `private` keywords to specify shared or private variables among processes\n",
      "\n",
      "These concepts are discussed on pages 15-18 of the provided document.\n"
     ]
    }
   ],
   "source": [
    "print(\"PDFs in data/:\", [p.name for p in DATA_DIR.glob(\"*.pdf\")])\n",
    "print(\"Indexed chunks:\", len((vectorstore.get() or {}).get(\"ids\", [])))\n",
    "\n",
    "# Example queries:\n",
    "ans = ask(\"List the key study tactics and exam strategies across all PDFs.\")\n",
    "print(ans)\n",
    "\n",
    "ans = ask_fresh(\"Summarize memory techniques mentioned.\")\n",
    "print(ans)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lc-ra-desktop)",
   "language": "python",
   "name": "lc-ra-desktop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
